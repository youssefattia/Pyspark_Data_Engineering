{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d079ab53-3eb3-49a8-9ada-a97df600bfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "from operator import add\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from tempfile import NamedTemporaryFile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bebe2c61-91d4-42e9-9872-d55b841c0b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"cite\") \\\n",
    ".config(\"spark.memory.fraction\", 0.8) \\\n",
    ".config(\"spark.executor.memory\", \"14g\") \\\n",
    ".config(\"spark.driver.memory\", \"12g\")\\\n",
    ".config(\"spark.sql.shuffle.partitions\" , \"800\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "execution_time_file = open('execution_time_file.txt','a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecdb94e2-f6f3-464c-a0e9-f921dcc38494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a line from the file papers_vocab.txt\n",
    "# comma to separate ID and vocab, space to separate vocabularies.\n",
    "def parse_papers_count(line):\n",
    "    if not line:\n",
    "        return dict()\n",
    "    papersCountRaw = line.split(' ')\n",
    "    papersCount = dict()\n",
    "    for pcRaw in papersCountRaw:\n",
    "        paper, count = pcRaw.split(':')\n",
    "        papersCount[paper] = int(count)\n",
    "    return papersCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcf2be8d-d189-45da-805a-d65b53b07a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a line from the file users_libraries.txt\n",
    "# semi-colon to separate user hash id with library, comma to separate the IDs in the library.\n",
    "def parse_users_libraries(line):\n",
    "    if not line:\n",
    "        return\n",
    "\n",
    "    userHash, libraryRAW = line.split(';')\n",
    "    library = [int(paper_id) for paper_id in libraryRAW.split(',')]\n",
    "    return userHash, library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d1e1fe8-34fc-44c5-801d-99d80b5eda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a line from the file papers.csv. Comma seperated. Each line has 15 fields.\n",
    "# The first is paper_id, the last is 'abstract' of a paper.\n",
    "def parse_papers(line):\n",
    "    if not line:\n",
    "        return\n",
    "    papersInfo = csv.reader([line.replace(\"\\0\", \"\")], delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "    papersInfoList = next(papersInfo)\n",
    "    # paper_id, abstract (the last element in the list)\n",
    "    return papersInfoList[0], papersInfoList[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654156ec-c0aa-4b82-af6e-605235b55e31",
   "metadata": {},
   "source": [
    "### Loading the data in users_libraries.txt into an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40f3a193-1932-423a-b504-a4423b514d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "users_libraries_path = 'users_libraries.txt'\n",
    "userRatingsRDD = sc.textFile(users_libraries_path)\n",
    "# key - user_hash id, values - list of paper_ids\n",
    "userRatingsRDD = userRatingsRDD.map(parse_users_libraries)\n",
    "\n",
    "execution_time_file.write('Loading of userRatingsRDD: --- %s seconds --- \\n' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adc41d1-48ee-47e6-a247-e7bad191793a",
   "metadata": {},
   "source": [
    "### Loading the data in papers.csv into an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de43f702-b547-4342-8a55-f1bd5160476e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "papers_path = 'papers.csv'\n",
    "# load paper terms \n",
    "paperTermsRDD = sc.textFile(papers_path)\n",
    "# key - paper_id, value - abstract\n",
    "allPaperTermsRDD = paperTermsRDD.map(parse_papers)\n",
    "\n",
    "# filter empty abstracts\n",
    "paperTermsRDD = allPaperTermsRDD.filter(lambda x: x[1] != '')\n",
    "\n",
    "# key - paper_id, values - list of words \n",
    "paperTermsRDD = paperTermsRDD.map(lambda x: (int(x[0]), x[1].lower().split(' ')))\n",
    "paperTermsRDD.take(10)\n",
    "\n",
    "execution_time_file.write('Loading of paperTermsRDD: --- %s seconds --- \\n' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afbe06d-d66c-48fa-8306-94abe3c7931e",
   "metadata": {},
   "source": [
    "We will now generate the top-10 most frequent words in the favorite papers for each user. \n",
    "We need to join the paperTermsRDD and userRatingsRDD such that the user hashes are linked directly with the words.\n",
    "After which we calculate the top words for each user.\n",
    "RDD's do not allow nested opperations and join is only possible by key on key. \n",
    "Therefore, we need to interchange the key and values of both usersLibraries RDD and PaperTerms RDD to paper_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffe68308-9bce-4938-897a-851437bf9b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a broadcast variable of stopwords\n",
    "# the variable is stored across all nodes\n",
    "# we need to remove stopwords from paper terms parallely in multiple nodes\n",
    "stopwordsBroadcast = sc.broadcast([word.rstrip('\\n') for word in open('stopwords_en.txt')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34c13a26-0f03-4a73-a587-cf275c8e3308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate the values from an array to separate rows\n",
    "def flatten(value):\n",
    "    return value\n",
    "\n",
    "# remove \n",
    "def filterStopWords(value):\n",
    "    return value[1] not in stopwordsBroadcast.value\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# First we flatten the paperTermsRDD and filter out the stop words [paper_id: word]\n",
    "flattenedPaperTermsRDD = paperTermsRDD.flatMapValues(flatten).filter(filterStopWords)\n",
    "\n",
    "# Same needs to be done for paper terms as well [user_hash: paper_id]\n",
    "flattenedUserRatingsRDD = userRatingsRDD.flatMapValues(flatten)\n",
    "# Now, we need to interchange the keys and values \n",
    "# we want to join the paperids from user ratings and paper terms RDD's [paper_id: user_hash]\n",
    "paperUsersRDD = flattenedUserRatingsRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# since nested RDD's are not supported, we need to create a unique userhash_word to word count mapping \n",
    "userWordsRDD = paperUsersRDD.join(flattenedPaperTermsRDD).map(lambda x: (x[1][0]+'_'+x[1][1], 1))\n",
    "# We now reduce the RDD by keys such that [userhash_word: count]\n",
    "userWordsRDD = userWordsRDD.reduceByKey(add)\n",
    "\n",
    "# retrieve the original user hashes as keys\n",
    "def split_hash_word(x):\n",
    "    userHash, word = x[0].split('_')\n",
    "    return (userHash, (word, x[1]))\n",
    "\n",
    "userWordCountsRDD = userWordsRDD.map(split_hash_word)\n",
    "\n",
    "# sort the word by their occurances and take the first 10\n",
    "sortedUserCountsRDD = userWordCountsRDD.groupByKey().mapValues(lambda x: sorted(list(x), key=lambda y: -y[1])[:10])\n",
    "\n",
    "#sortedUserCountsRDD.saveAsTextFile(tempFile.name)\n",
    "execution_time_file.write('Joining RDDs and retriveing top 10 words: --- %s seconds --- \\n' % (time.time() - start_time)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688c315c-0352-413b-a6f8-c0cbc5e9ba75",
   "metadata": {},
   "source": [
    "We will now perform some data analysis on the RDDs\n",
    "\n",
    "### Number of (distinct) user, number of (distinct) items, and number of ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01cfe740-4cd2-4f21-b114-0e0955b54d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users (RDD) 28416\n",
      "\n",
      "number of items (RDD) 172079\n",
      "\n",
      "number of ratings (RDD) 28416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "n_users = userRatingsRDD.keys().distinct().count()\n",
    "print('number of users (RDD) %d\\n' % (n_users))\n",
    "\n",
    "# we take into account even papers with empty abstracts to calculate the list of items\n",
    "n_items = allPaperTermsRDD.keys().distinct().count()\n",
    "print('number of items (RDD) %d\\n' % n_items)\n",
    "\n",
    "n_ratings = flattenedUserRatingsRDD.keys().distinct().count()\n",
    "print('number of ratings (RDD) %d\\n' %n_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f8eb5a-fd7d-4b8d-ae6d-54dcff1cb77c",
   "metadata": {},
   "source": [
    "### Min, Max, average and standard deviation of number of ratings a user has given\n",
    "We create an RDD of user hash with the number of ratings given by user. MapValues is used to map the values (list of ratings) to the number of ratings (length of the list of ratings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50f03ab4-4400-434f-9a04-af5b66988a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum number of ratings (RDD) given by a user 1\n",
      "\n",
      "maximum number of ratings (RDD) given by a user 1922\n",
      "\n",
      "average number of ratings (RDD) given by a user 29\n",
      "\n",
      "standard deviation of number of ratings (RDD) given by a user 81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "userRatingNumberRDD = userRatingsRDD.mapValues(len).values()\n",
    "\n",
    "min_user_ratings = userRatingNumberRDD.min()\n",
    "print('minimum number of ratings (RDD) given by a user %d\\n' % min_user_ratings)\n",
    "\n",
    "max_user_ratings = userRatingNumberRDD.max()\n",
    "print('maximum number of ratings (RDD) given by a user %d\\n' % max_user_ratings)\n",
    "\n",
    "avg_user_ratings = userRatingNumberRDD.mean()\n",
    "print('average number of ratings (RDD) given by a user %d\\n' % avg_user_ratings)\n",
    "\n",
    "std_dev_user_ratings = userRatingNumberRDD.stdev()\n",
    "print('standard deviation of number of ratings (RDD) given by a user %d\\n' % std_dev_user_ratings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87013b5d-b066-452b-940b-5ed9a35c42c7",
   "metadata": {},
   "source": [
    "### Mean, max, average and standard deviation of ratings an item has recieved\n",
    "We create a grouped RDD on the individual papers (items). The ratings corresponding to each distinct paper_id are grouped together. We again use mapValues to map the list of ratings to their length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb80be83-ff15-4566-9e98-c33bdd4d24dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum number of ratings (RDD) recieved by the item (paper) 3\n",
      "\n",
      "maximum number of ratings (RDD) recieved by the item (paper) 924\n",
      "\n",
      "average number of ratings (RDD) recieved by the item (paper) 4\n",
      "\n",
      "standard deviation of a number of ratings (RDD) recieved by the item (paper) 5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paperRatingNumberRDD = paperUsersRDD.groupByKey().mapValues(len).values()\n",
    "\n",
    "min_paper_ratings = paperRatingNumberRDD.min()\n",
    "print('minimum number of ratings (RDD) recieved by the item (paper) %d\\n' % min_paper_ratings)\n",
    "\n",
    "max_paper_ratings = paperRatingNumberRDD.max()\n",
    "print('maximum number of ratings (RDD) recieved by the item (paper) %d\\n' %max_paper_ratings)\n",
    "\n",
    "avg_paper_ratings = paperRatingNumberRDD.mean()\n",
    "print('average number of ratings (RDD) recieved by the item (paper) %d\\n' %avg_paper_ratings)\n",
    "\n",
    "std_dev_paper_ratings = paperRatingNumberRDD.stdev()\n",
    "print('standard deviation of a number of ratings (RDD) recieved by the item (paper) %d\\n' %std_dev_paper_ratings)\n",
    "\n",
    "execution_time_file.write('Data Analysis over RDDs : --- %s seconds --- \\n' % (time.time() - start_time)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eacad79-93d7-483a-9fe4-360e1365fa7f",
   "metadata": {},
   "source": [
    "## Loading the Data onto Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46c99fef-b918-40ab-b690-9a88792da4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# loading the schema as mentioned in the README \n",
    "papersSchema = StructType([\n",
    "    StructField(\"paper_id\", IntegerType(), False),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"journal\", StringType(), True),\n",
    "    StructField(\"book_title\", StringType(), True),\n",
    "    StructField(\"series\", StringType(), True),\n",
    "    StructField(\"publisher\", StringType(), True),\n",
    "    StructField(\"pages\", IntegerType(), True),\n",
    "    StructField(\"volume\", IntegerType(), True),\n",
    "    StructField(\"number\", IntegerType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"month\", IntegerType(), True),\n",
    "    StructField(\"postedat\", IntegerType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"abstract\", StringType(), True)\n",
    "])\n",
    "# load csv data onto the RDD\n",
    "allPapersDf = spark.read.csv(papers_path, header = False, schema = papersSchema).select('paper_id', 'abstract')\n",
    "# drop rows with empty abstract \n",
    "papersAbstractDf = allPapersDf.dropna()\n",
    "\n",
    "# loading the users usersLibraries schema as mentioned in the README\n",
    "usersLibrariesSchema = StructType([\n",
    "    StructField(\"user_hash_id\", StringType(), False),\n",
    "    StructField(\"user_library\", StringType(), False)\n",
    "])\n",
    "# load data into dataframe\n",
    "usersLibrariesDf = spark.read.csv(users_libraries_path, sep = \";\", header = False, schema = usersLibrariesSchema)\n",
    "execution_time_file.write('Loading Data into dataframes : --- %s seconds --- \\n' % (time.time() - start_time)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19230c9a-6064-4026-9980-240480a1e66c",
   "metadata": {},
   "source": [
    "We will now use dataframes to generate the top-10 most frequent words in the favorite papers for each user. \n",
    "This task is simpler to accomplish with Dataframes than with RDD's as we can join on any column and hence don't have to reverse the key-value pairs. Dataframes also provide several functions that can be operated on different values. The two functions relevant for our result are split and explode.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e194f9cb-d1c3-4f33-b021-5666c14181ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# split the usersLibraries as an array and store in column with name user_library_arr [user_hash_id, user_library_arr]\n",
    "usersLibrariesDf = usersLibrariesDf.select(usersLibrariesDf.user_hash_id,\\ \n",
    "                                           F.split(usersLibrariesDf.user_library,',').alias('user_library_arr'))\n",
    "\n",
    "# explode is used to separate the paper_ids from the user_library_arr list into separate rows (opposite of groupBy) [user_hash_id, paper_id]\n",
    "usersPapersDf = usersLibrariesDf.select(usersLibrariesDf.user_hash_id,\\ \n",
    "                                        F.explode(usersLibrariesDf.user_library_arr).alias('paper_id'))\n",
    "\n",
    "# split the words in the abstract of the papersAbstractDf as an array in the column abstract_words [paper_id, abstract_words]\n",
    "papersAbstractDf = papersAbstractDf.select(papersAbstractDf.paper_id,\\ \n",
    "                                           F.split(papersAbstractDf.abstract, ' ').alias('abstract_words'))\n",
    "\n",
    "# join the usersPapersDf and papersAbstractDf on the column 'paper_id' [user_hash_id, abstract_words]\n",
    "usersWordsDf = usersPapersDf.join(papersAbstractDf, usersPapersDf.paper_id == papersAbstractDf.paper_id)\\\n",
    "                            .select(usersPapersDf.user_hash_id, papersAbstractDf.abstract_words)\n",
    "\n",
    "# using explode, separate the words of the abstract into separate words [user_hash_id, words]\n",
    "usersWordsDf = usersWordsDf.select(usersWordsDf.user_hash_id, \\\n",
    "                                   F.explode(usersWordsDf.abstract_words).alias('word'))\n",
    "\n",
    "# The words need to be transformed into lower case before filtering out the stopwords\n",
    "usersWordsDf = usersWordsDf.select(usersWordsDf.user_hash_id, \\\n",
    "                                   F.lower(F.col('word')).alias('word'))\n",
    "# Filter out stopwords \n",
    "usersWordsDf = usersWordsDf.filter(~usersWordsDf.word.isin(stopwordsBroadcast.value))\n",
    "\n",
    "# using aggregate, calcuate the count of all the rows in the group formed by user_hash_id and word combined.\n",
    "usersWordCountsDf = usersWordsDf.groupBy(usersWordsDf.user_hash_id, usersWordsDf.word)\\\n",
    "                                .agg(F.count('*').alias('word_count'))\n",
    "\n",
    "# create partions formed by different user_hash_ids and inside the partitions we order by the word count in descending order\n",
    "usersWordCountsWindow = Window.partitionBy(usersWordCountsDf.user_hash_id)\\\n",
    "                                .orderBy(usersWordCountsDf.word_count.desc())\n",
    "\n",
    "# using the rank window function, create a column top_word_rank containing the rank of the word wrt its word count in the wondow (partition of hash_id)\n",
    "topUsersWordsDf = usersWordCountsDf.withColumn('top_word_rank', \\\n",
    "                                               F.rank().over(usersWordCountsWindow))\\\n",
    "                                               .filter(F.col('top_word_rank') <= 10)\n",
    "\n",
    "# The result is generated by aggregating the remaining top words into a list of top_words\n",
    "topUsersWordsDf = topUsersWordsDf.groupBy(usersWordCountsDf.user_hash_id)\\\n",
    "                                .agg(F.collect_list(topUsersWordsDf.word)\\\n",
    "                                .alias('top_words'))\n",
    "\n",
    "execution_time_file.write('Calculating top words in dataframes : --- %s seconds --- \\n' % (time.time() - start_time)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432c0be6-0028-4cff-b5ab-00c1cefb81fa",
   "metadata": {},
   "source": [
    "### Data Analysis on top of DataFrames\n",
    "#### Number of distinct users, items and ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c78d9a9-8f38-4332-8dd3-3a6c00dfcded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users (dataframe) 28416\n",
      "\n",
      "number of items (paper_ids) (dataframe) 172079\n",
      "\n",
      "number of ratings (dataframe) 828481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "n_users = usersLibrariesDf.distinct().count()\n",
    "print('number of users (dataframe) %d\\n' %n_users)\n",
    "\n",
    "n_items = allPapersDf.distinct().count()\n",
    "print('number of items (paper_ids) (dataframe) %d\\n' %n_items)\n",
    "\n",
    "n_ratings = usersPapersDf.count()\n",
    "print('number of ratings (dataframe) %d\\n' %n_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bba94a-014a-4971-9c8e-30099f2a6511",
   "metadata": {},
   "source": [
    "#### Min, Max, average and standard deviation of number of ratings a user has given\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a18ee0f7-02ee-4134-ac6f-4e6a5b105332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+------------------+-------------------+\n",
      "|min_user_ratings|max_user_ratings|  avg_user_ratings|stddev_user_ratings|\n",
      "+----------------+----------------+------------------+-------------------+\n",
      "|               1|            1922|29.155440596846848|  81.17660451011598|\n",
      "+----------------+----------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we create a column n_ratings with count of ratings = length of the user_library_arr list\n",
    "nRatingsDf = usersLibrariesDf.withColumn('n_ratings', F.size(usersLibrariesDf.user_library_arr))\n",
    "\n",
    "# aggregator function to calculate the statistics on number of ratings user has given\n",
    "nRatingsDf.agg(F.min('n_ratings').alias('min_user_ratings'), \n",
    "               F.max('n_ratings').alias('max_user_ratings'), \n",
    "               F.mean('n_ratings').alias('avg_user_ratings'),  \n",
    "               F.stddev('n_ratings').alias('stddev_user_ratings')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ea084-eebb-4efa-b1d3-886ec625b501",
   "metadata": {},
   "source": [
    "#### Mean, max, average and standard deviation of ratings an item has recieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c592cfa-834d-4c1c-8684-bea660411ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+----------------+-------------------+\n",
      "|min_item_ratings|mav_item_ratings|avg_item_ratings|stddev_item_ratings|\n",
      "+----------------+----------------+----------------+-------------------+\n",
      "|               3|             924|4.81453867119172|  5.477818208917296|\n",
      "+----------------+----------------+----------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the counts of each group containing distinct paper_ids\n",
    "nPaperRatingsDf = usersPapersDf.groupBy('paper_id').count()\n",
    "\n",
    "# aggregator function to calculate the statistics on the number of ratings an item has recieved\n",
    "nPaperRatingsDf.agg(F.min('count').alias('min_item_ratings'), \n",
    "                    F.max('count').alias('mav_item_ratings'), \n",
    "                    F.mean('count').alias('avg_item_ratings'),\n",
    "                    F.stddev('count').alias('stddev_item_ratings')).show()\n",
    "\n",
    "execution_time_file.write('Data Analysis over Dataframes : --- %s seconds --- \\n' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c09d57-9ead-4f05-b5dc-bee86db4e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_time_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
